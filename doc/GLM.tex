\documentclass[12pt]{article}

\usepackage[titletoc]{appendix}

\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{multicol}

\usepackage[margin=1in]{geometry}

\title{Generalized Linear Models (GLM)}
\author{Ludovic Stourm}

\begin{document}
\maketitle

% TO DO: add intro

\section{Univariate generalized linear model}
\subsection{General setup}
Model:
\begin{equation}
	Y_n \sim \mathcal{F}(V_n) \hspace*{25pt} \text{where } \hspace*{5pt} V_n = X_n \beta
\end{equation}
where $Y_n$ and $V_n$ are scalars, $\mathcal{F}$ is some probability distribution, $X_n$ is a $[1 \times L]$ vector of observables, and $\beta$ is a $[L \times 1]$ vector of parameters to estimate.

\begin{equation}
\begin{aligned}
\text{Likelihood:} \\
	& LL(\beta) && = \sum_n f(V_n) \\
	&  && = \boxed{\mathbbm{1}_N \cdot \textbf{f}}  && \text{where} \hspace*{5pt} f_n = f(V_n) \\
\text{Gradient:} \\
	& \frac{\partial LL}{\partial \beta_l} && = \sum_n X_{nl} \frac{\partial f(V_n)}{\partial V_n} \\
	& \nabla LL(\beta) && = \boxed{\textbf{X}' \textbf{g}} && \text{where} \hspace*{5pt} g_n = \frac{\partial f(V_n)}{\partial V_n} \\
\text{Hessian:} \\
	& \frac{\partial^2 LL}{\partial \beta_l \partial \beta_{}} && = \sum_n X_{nl} X_{nl'} \frac{\partial^2 f(V_n)}{\partial V_n^2} \\
	& \textbf{H}(\beta) && = \boxed{\textbf{X}' \left[ \textbf{X} \odot \left(\textbf{h} \cdot \mathbbm{1}_P \right) \right]}  && \text{where} \hspace*{5pt} h_n = \frac{\partial^2 f(V_n)}{\partial V_n^2} \\
\end{aligned}
\end{equation}
where $\textbf{X}$ is a matrix of dimensions $[N \times K]$, $\beta$ is a vector of dimensions $[K \times 1]$, $\textbf{f}$, $\textbf{g}$ and $\textbf{h}$ are vectors of dimensions $[N \times 1]$, $\mathbbm{1}_Q$ is a vector of ones of dimension $[1 \times Q]$, and $\odot$ represents the Hadamard product (term-by-term multiplication).

\subsection{Examples}
\subsubsection{Model 1: Linear model}
Model:
\begin{equation}
	Y_n \sim N(V_n, \sigma^2)
\end{equation}
Link function and derivatives:
\begin{equation}
\left\{ \begin{aligned}
	& f_n && = -\frac{1}{2 \sigma^2} (y_n -V_n)^2 \ \\
	& g_n && =  \frac{1}{\sigma^2} (y_n -V_n) \\
	& h_n && =  -\frac{1}{\sigma^2} \\
\end{aligned} \right.
\end{equation}
The gradient is equal to zero when:
\begin{equation}
\begin{aligned}
	&\sum_n X_{nl} (y_n - X_n \beta) && = 0 \hspace*{5pt} \forall l  \\
	\implies & \sum_n X_{nl} y_n && = \sum_n X_{nl} X_n \beta \hspace*{5pt} \forall l  \\
	\implies & \textbf{X}' \textbf{Y} && = \textbf{X}'(\textbf{X} \beta) \\
	\implies & \beta && =  (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{Y} \\
\end{aligned}
\end{equation}

\subsubsection{Model 2: Poisson count model / Exponential duration model}
Model:
\begin{equation}
\begin{aligned}
	& Y_n && \sim Poisson(\lambda_n) \\
	& \log(\lambda_n) && = C_n + V_n
\end{aligned}
\end{equation}
Link function and derivatives:
\begin{equation}
\left\{ \begin{aligned}
	& f_n && =  - \lambda_n + y_n V_n + K_n  \\
	& && \text{where } K_n =  y_n C_n - \log(y_n!) \hspace*{5pt} \text{does not depend on } \beta \\
	& g_n && =  - \lambda_n + y_n \\
	& h_n && =  - \lambda_n \\
\end{aligned} \right.
\end{equation}
\underline{Remarks}:
\begin{itemize}
	\item This subsumes a Poisson count process such that $Y_n$ counts the number of events arising at rate $\mu_n$ within a time interval of duration $D_n$. In that case:
	\begin{equation}
	\begin{aligned}
		& Y_n && \sim Poisson(D_n \mu_n) \\
		& \log(\mu_n) && = A_n + V_n \\
	\rightarrow \text{Define: }	& \lambda_n && = D_n \mu_n \\
	\text{and: }	& C_n && = \log(D_n) + A_n \\
	\end{aligned}
	\end{equation}
	
	\item By duality of the Poisson process and the Exponential duration model, this subsumes a duration model with constant hazard rate and potential truncation (up to a normalizing constant). In that case: 
	\begin{equation}
	\begin{aligned}
			& T_n^* && \sim Exponential(\mu_n) \\
			& \log(\mu_n) && = C_n + V_n \\
			& T_n && = \min\{T_n^*, T_{\max}\} \\
	\rightarrow \text{Define: }	& Y_n && = \mathbbm{1}\{T_n = T_n^*\} \hspace*{5pt} (\text{indicates whether the event occurs within } [0, T_{\max}] ) \\
	\text{and: }	& \lambda_n && = \mu_n T_n \\
	\text{Then: } & L_n &&= \mu_n^{y_n} e^{-\mu_n T_n} = \left(1/T_n\right)^{y_n} \lambda_n^{y_n} e^{-\lambda_n} \\
	& f_n && = - \lambda_n + y_n V_n + K_n^* \\
	& && \text{where } K_n^* =  y_n C_n \hspace*{5pt} \text{does not depend on } \beta \\
	\end{aligned}
	\end{equation}
	Thus, only the constant $K_n^*$ is different.
\end{itemize}

\subsubsection{Model 3: Binomial logit / Logistic regression}
Model:
\begin{equation}
\begin{aligned}
	& Y_n && \sim Multinomial(M_n, p_n) \\
	& p_n && = 1 / \left( 1 + \exp[-V_n] \right)
\end{aligned}
\end{equation}
Link function and derivatives:
\begin{equation}
\left\{ \begin{aligned}
	& f_n && =  y_n V_n - M_n \log\left( 1 + \exp[V_n] \right) + K_n \\
	&  && =  y_n \log(p_n) + (M_n - y_n) \log\left( 1 - p_n \right) + K_n \\
		& && \text{where } K_n = \log\left( M_n ! / \left[ y_n ! (M_n - y_n)! \right] \right) \hspace*{5pt} \text{does not depend on } \beta \\
	& g_n && =  y_n - M_n p_n \\
	& h_n && =  - M_n p_n (1-p_n) \\
\end{aligned} \right.
\end{equation}


\subsection{Estimation by Maximum Likelihood}
Newton-Raphson:
\begin{equation}
	\beta^{(i+1)} \leftarrow \beta^{(i)} - \left[H(\beta)\right]^{-1} \nabla_{LL}(\beta)
\end{equation}

Here are the things that need to be computed efficiently to estimate the model:
\begin{itemize}
	\item $\textbf{V} = \textbf{X} \beta $
	\item $\nabla LL(\beta) = \textbf{X}' \textbf{g}$
	\item $\textbf{X}' \left[ \textbf{X} \odot \left(\textbf{h} \cdot \mathbbm{1}_P \right) \right]$
	\item $\textbf{X}' \textbf{X} $ (special case of the previous line, where \textbf{h} is a column of ones.
\end{itemize}

\subsubsection{Case with two dimensions of variation}
The matrix of covariates $\textbf{X}$ is of dimensions $[N \times P]$. We consider the case when the data varies across two dimensions with subscripts $(i,j)$, such that the data is balanced in the sense that all combinations of ($i,j$) appear exactly once (in which case $N = I \times J$). Furthermore, some of the covariates $X$ vary only according to $i$ (and are constant across $j$), and some covariates vary only according to $j$ (and are constant across $i$). In that case, we can avoid the full expansion of matrix $X$ (which may require a large chunk of memory), and perform computations more efficiently.

We split the matrix of covariates $X_{ij}$ into the three corresponding groups to obtain matrices $X^{(1)}_{ij}$, $X^{(2)}_{i}$, $X^{(3)}_{j}$, and we denote by $\beta^{(1)}, \beta^{(2)}, \beta^{(3)}$ the corresponding subvectors of $\beta$.

\begin{itemize}
\item To compute $\textbf{V} = \textbf{X}\beta$:
\begin{equation}
\begin{aligned}
	& V_{ij} && = X_{ij} \beta \\
	& && = X^{(1)}_{ij} \beta^{(1)}  + X^{(2)}_{i} \beta^{(2)} + X^{(3)}_{j} \beta^{(3)}
\end{aligned}
\end{equation}

\item To compute $\nabla LL(\beta) = \textbf{X}' \textbf{g}$:
	\begin{equation}
	\begin{aligned}
		& \nabla LL^{(1)} && = \sum_{i} \sum_{j} g_{ij} X^{(1)}_{ij}			 \\
		& \nabla LL^{(2)} && = \sum_{i} \left[ \sum_{j} g_{ij} \right] X^{(2)}_{i}		&& = \sum_{i}  g^{(2)}_{i} X^{(2)}_{i}			&& \text{where} && g^{(2)}_{i} && = \sum_{j} g_{ij}  \\
		& \nabla LL^{(3)} && = \sum_{j} \left[ \sum_{i} g_{ij} \right] X^{(3)}_{j}		&& = \sum_{j} g^{(3)}_{j} X^{(3)}_{j}			&& \text{where} && g^{(3)}_{j} && = \sum_{i} g_{ij}  \\	\end{aligned}
	\end{equation}

\item To compute $\textbf{X}' \left[ \textbf{X} \odot \left(\textbf{h} \cdot \mathbbm{1}_P \right) \right]$:
	\begin{equation*}
	\begin{aligned}
		& H^{(1,1)} && = \sum_{i} \sum_{j} h_{ij} X^{(1)}_{ij} X^{(1)}_{ij} \\
		& H^{(1,2)} && = \sum_{i} \left[  \sum_{j} h_{ij} X^{(1)}_{ij} \right] X^{(2)}_{i} \\
		& H^{(1,3)} && = \sum_{j} \left[ \sum_{i} h_{ij} X^{(1)}_{ij} \right] X^{(3)}_{j} \\
		& H^{(2,2)} && = \sum_{i} \left[ \sum_{j} h_{ij} \right] X^{(2)}_{i} X^{(2)}_{i} \\
		& H^{(2,3)} && = \sum_{j} \left[ \sum_{i} h_{ij} X^{(2)}_{i} \right] X^{(3)}_{j} \\
		& H^{(3,3)} && = \sum_{j} \left[ \sum_{i} h_{ij} \right] X^{(3)}_{j} X^{(3)}_{j} \\
		& H^{(i,j)} && = H^{(j,i)} \hspace*{15pt} \text{if } i > j \\
	\end{aligned}
	\end{equation*}

\end{itemize}

\subsubsection{Case with three dimensions of variation}
Similarly, let us now consider the case when the data varies across three dimensions with subscripts $(i,j,k)$, such that the data is balanced in the sense that all combinations of ($i,j,k$) appear exactly once (in which case $N = I \times J \times K$). We similarly split the matrix of covariates $X_{ijk}$ into six groups, as a function of their dimension(s) of variations, to obtain matrices $X^{(1)}_{ijk}$, $X^{(2)}_{ij}$, $X^{(3)}_{ik}$, $X^{(4)}_{jk}$, $X^{(5)}_{i}$, $X^{(6)}_{j}$, $X^{(7)}_{k}$, and we denote by $\beta^{(1)}, \beta^{(2)}, \beta^{(3)}, \beta^{(4)}, \beta^{(5)}, \beta^{(6)}, \beta^{(7)}$ the corresponding subvectors of $\beta$.

\begin{itemize}
\item To compute $\textbf{V} = \textbf{X}\beta$:
\begin{equation}
\begin{aligned}
	& V_{ijk} && = X_{ijk} \beta \\
	& && = X^{(1)}_{ijk} \beta^{(1)} + X^{(2)}_{ij} \beta^{(2)} + X^{(3)}_{ik} \beta^{(3)} + X^{(4)}_{jk} \beta^{(4)} + X^{(5)}_{i} \beta^{(5)} + X^{(6)}_{j} \beta^{(6)} + X^{(7)}_{k} \beta^{(7)} 
\end{aligned}
\end{equation}

\item To compute $\nabla LL(\beta) = \textbf{X}' \textbf{g}$:
	\begin{equation}
	\begin{aligned}
		& \nabla LL^{(1)} && = \sum_{i} \sum_{j} \sum_{k} g_{ijk} X^{(1)}_{ijk}		  \\
		& \nabla LL^{(2)} && = \sum_{i} \sum_{j} \left[\sum_{k} g_{ijk} \right] X^{(2)}_{ij}		&& = \sum_{i} \sum_{j} g^{(2)}_{ij} X^{(2)}_{ij}	&& \text{where} && g^{(2)}_{ij} && = \sum_{k} g_{ijk}  \\
		& \nabla LL^{(3)} && = \sum_{k} \sum_{i} \left[ \sum_{j} g_{ijk} \right] X^{(3)}_{ik}	&& = \sum_{k} \sum_{i} g^{(3)}_{ik} X^{(3)}_{ik}	&& \text{where} && g^{(3)}_{ik} && = \sum_{j} g_{j}  \\		
		& \nabla LL^{(4)} && = \sum_{k} \sum_{j} \left[ \sum_{i}  g_{ijk} \right] X^{(4)}_{jk}	&& = \sum_{k} \sum_{j} g^{(4)}_{jk} X^{(4)}_{jk}	&& \text{where} && g^{(4)}_{jk} && = \sum_{i} g_{ijk}  \\
		& \nabla LL^{(5)} && = \sum_{i} \left[ \sum_{k} \sum_{j} g_{ijk} \right] X^{(5)}_{i}		&& = \sum_{i}  g^{(5)}_{i} X^{(5)}_{i}				&& \text{where} && g^{(5)}_{i} && = \sum_{j,k} g_{ijk}  \\
		& \nabla LL^{(6)} && = \sum_{j} \left[ \sum_{k} \sum_{i} g_{ijk} \right] X^{(6)}_{j}		&& = \sum_{j} g^{(6)}_{j} X^{(6)}_{j}			&& \text{where} && g^{(6)}_{j} && = \sum_{i,k} g_{ijk}  \\		& \nabla LL^{(7)} && = \sum_{k} \left[ \sum_{i} \sum_{j} g_{ijk} \right] X^{(7)}_{k}		&& = \sum_{k}  g^{(7)}_{k} X^{(7)}_{k}				&& \text{where} && g^{(7)}_{k} && = \sum_{i,j} g_{ijk}  \\
	\end{aligned}
	\end{equation}
	Thus, we first compute $g^{(1)}_{ijk}$, $g^{(2)}_{ij}$, $g^{(3)}_{ik}$, $g^{(4)}_{jk}$, $g^{(5)}_{i}$, $g^{(6)}_{j}$, $g^{(7)}_{k}$. Then, we multiply them with the corresponding matrices $X^{(2)}_{ij}$, $X^{(3)}_{ik}$, $X^{(4)}_{jk}$, $X^{(5)}_{i}$, $X^{(6)}_{j}$, $X^{(7)}_{k}$. We combine all gradient sub-vectors together to obtain the gradient with respect to the full vector of parameters $\beta$. Following this process allows us to avoid the full expansion of $X$ at the $(i,j,k)-$ level.

\item To compute $\textbf{X}' \left[ \textbf{X} \odot \left(\textbf{h} \cdot \mathbbm{1}_P \right) \right]$:
\clearpage
\begin{multicols}{2}
	\begin{equation*}
	\begin{aligned}
		& H^{(1,1)} && = \sum_{i} \sum_{j} \sum_{k} h_{ijk} X^{(1)}_{ijk} X^{(1)}_{ijk} \\
		& H^{(1,2)} && = \sum_{i} \sum_{j} \left[ \sum_{k} h_{ijk} X^{(1)}_{ijk} \right] X^{(2)}_{ij} \\
		& H^{(1,3)} && = \sum_{i} \sum_{k} \left[ \sum_{j} h_{ijk} X^{(1)}_{ijk} \right] X^{(3)}_{ik} \\
		& H^{(1,4)} && = \sum_{j} \sum_{k} \left[ \sum_{i} h_{ijk} X^{(1)}_{ijk} \right] X^{(4)}_{jk} \\
		& H^{(1,5)} && = \sum_{i} \left[ \sum_{j} \sum_{k} h_{ijk} X^{(1)}_{ijk} \right] X^{(5)}_{i} \\
		& H^{(1,6)} && = \sum_{j} \left[ \sum_{i} \sum_{k} h_{ijk} X^{(1)}_{ijk} \right] X^{(6)}_{j} \\
		& H^{(1,7)} && = \sum_{k} \left[ \sum_{i} \sum_{j} h_{ijk} X^{(1)}_{ijk} \right] X^{(7)}_{k} \\
		& H^{(2,2)} && = \sum_{i} \sum_{j} \left[\sum_{k} h_{ijk} \right] X^{(2)}_{ij} X^{(2)}_{ij} \\
		& H^{(2,3)} && = \sum_{k} \sum_{i} \left[ \sum_{j} h_{ijk} X^{(2)}_{ij} \right] X^{(3)}_{ik} \\
		& H^{(2,4)} && = \sum_{k} \sum_{j} \left[ \sum_{i}  h_{ijk} X^{(2)}_{ij} \right] X^{(4)}_{jk} \\
		& H^{(2,5)} && = \sum_{i} \left[  \sum_{j} \left( \sum_{k} h_{ijk} \right) X^{(2)}_{ij} \right] X^{(5)}_{i} \\
		& H^{(2,6)} && = \sum_{j} \left[ \sum_{i} \left( \sum_{k} h_{ijk} \right) X^{(2)}_{ij} \right] X^{(6)}_{j} \\
		& H^{(2,7)} && = \sum_{k} \left[ \sum_{i} \sum_{j} h_{ijk} X^{(2)}_{ij} \right] X^{(7)}_{k} \\
		& H^{(3,3)} && = \sum_{k} \sum_{i} \left[ \sum_{j} h_{ijk} \right] X^{(3)}_{ik} X^{(3)}_{ik} \\
		& H^{(3,4)} && = \sum_{k} \sum_{j} \left[ \sum_{i}  h_{ijk} X^{(3)}_{ik} \right] X^{(4)}_{jk} \\
	\end{aligned}
	\end{equation*}
	\columnbreak
	\begin{equation*}
	\begin{aligned}
		& H^{(3,5)} && = \sum_{i} \left[ \sum_{k} \left( \sum_{j} h_{ijk} \right) X^{(3)}_{ik} \right] X^{(5)}_{i} \\
		& H^{(3,6)} && = \sum_{j} \left[ \sum_{k} \sum_{i} h_{ijk} X^{(3)}_{ik} \right] X^{(6)}_{j} \\
		& H^{(3,7)} && = \sum_{k} \left[ \sum_{i} \left( \sum_{j} h_{ijk} \right) X^{(3)}_{ik} \right] X^{(7)}_{k} \\
		& H^{(4,4)} && = \sum_{k} \sum_{j} \left[ \sum_{i}  h_{ijk} \right] X^{(4)}_{jk} X^{(4)}_{jk} \\
		& H^{(4,5)} && = \sum_{i} \left[ \sum_{k} \sum_{j} h_{ijk} X^{(4)}_{jk} \right] X^{(5)}_{i} \\
		& H^{(4,6)} && = \sum_{j} \left[ \sum_{k} \left( \sum_{i} h_{ijk} \right) X^{(4)}_{jk} \right] X^{(6)}_{j} \\
		& H^{(4,7)} && = \sum_{k} \left[ \sum_{j} \left( \sum_{i} h_{ijk} \right) X^{(4)}_{jk} \right] X^{(7)}_{k}  \\
		& H^{(5,5)} && = \sum_{i} \left[ \sum_{k} \sum_{j} h_{ijk} \right] X^{(5)}_{i} X^{(5)}_{i} \\
		& H^{(5,6)} && = \sum_{j} \left[ \sum_{i} \left( \sum_{k} h_{ijk} \right) X^{(5)}_{i} \right] X^{(6)}_{j} \\
		& H^{(5,7)} && = \sum_{i} \left[ \sum_{k}  \left( \sum_{j} h_{ijk} \right) X^{(7)}_{k} \right] X^{(5)}_{i} \\
		& H^{(6,6)} && = \sum_{j} \left[ \sum_{k} \sum_{i} h_{ijk} \right] X^{(6)}_{j} X^{(6)}_{j} \\
		& H^{(6,7)} && = \sum_{j} \left[ \sum_{k} \left( \sum_{i} h_{ijk} \right) X^{(7)}_{k} \right] X^{(6)}_{j} \\
		& H^{(7,7)} && = \sum_{k} \left[ \sum_{i} \sum_{j} h_{ijk} \right] X^{(7)}_{k} X^{(7)}_{k} \\
		& H^{(i,j)} && = H^{(j,i)} \hspace*{15pt} \text{if } i > j \\
	\end{aligned}
	\end{equation*}
\end{multicols}

\end{itemize}


\section{Multivariate generalized linear model}
\subsection{Setup}
Model:
\begin{equation}
	\textbf{Y}_n \sim \mathcal{F}(\textbf{V}_n) \hspace*{25pt} \text{where } \hspace*{5pt} \textbf{V}_n = \textbf{X}_n \beta
\end{equation}
where $\textbf{Y}_n$ and $\textbf{V}_n$ are vectors, $\mathcal{F}$ is some probability distribution, $\textbf{X}_n$ is a $[J \times L]$ vector of observables, and $\beta$ is a $[L \times 1]$ vector of parameters to estimate.


\subsection{Multinomial Logit Model}
\paragraph{Notations:}
\begin{itemize}
	\item $\textbf{Y}_n$ is a $[J \times 1]$ vector such that $Y_{nj} \in \{0,1\}$ for all $j$
	\item $M_n = \sum_j \textbf{Y}_{nj}$ is the number of Multinomial trials
	\item $\textbf{X}_n$ is a $[J \times L]$ matrix of covariates
	\item $\beta$ is a $[L \times 1]$ matrix of parameters
	\item $\textbf{V}_{n}$ is a $[J \times 1]$ vector of utilities that defines outcome probabilities $\textbf{p}_n$
	\item $\textbf{p}_n$ is a $[J \times 1]$ vector of probabilities such that $\sum_j p_{nj} = 1$ and $0 \le p_{nj} \le 1$ for all $j$
	\item $\textbf{y}$ is a $[NJ \times 1]$ vector that stacks up all values $y_{nj}$
	\item $\textbf{p}$ is a $[NJ \times 1]$ vector that stacks up all values $p_{nj}$
	\item $\textbf{logp}$ is a $[NJ \times 1]$ vector that stacks up all values $\log(p_{nj})$
	\item $\textbf{M}$ is a $[N \times 1]$ vector that collects the values $M_n$
	\item $\tilde{\textbf{M}}$ is a $[NJ \times 1]$ vector that repeats the values $M_n$, such that $\tilde{M}_{nj} = M_n$ for all $j$
	\item $\textbf{A}$ is a $[N \times L]$ matrix such that $A_{nl} = \sum_j p_{nj} X_{njl}$
\end{itemize}

\paragraph{Model:}
\begin{equation}
\begin{aligned}
	& \textbf{Y}_n && \sim Multinomial(M_n, \textbf{p}_n) \\
	& p_{nj} && = \exp(V_{nj}) \big/ \sum_k \exp(V_{nk}) \hspace*{5pt} \text{for all } j \\
	& \textbf{V}_{n} && = \textbf{X}_n \beta
\end{aligned}
\end{equation}

\paragraph{Log-likelihood: \\}
\begin{equation}
\begin{aligned}
	& LL && = \sum_{j=1}^J y_{nj} \log(p_{nj}) + C = \boxed{ \textbf{y}' \textbf{logp} + C } \\
	\text{where } \hspace*{5pt} & C && =\sum_n \left[ \log(M_n!) - \sum_{j=1}^J \log(y_{nj}!) \right]
\end{aligned}
\end{equation}

\paragraph{Gradient of log-likelihood: \\}
\begin{equation}
\begin{aligned}
	& \frac{\partial LL}{\partial \beta_l} && = \sum_n \sum_{j=1}^J X_{njl} (y_{nj} - M_n p_{nj})
\implies & \nabla LL(\beta) && = \boxed{\textbf{X}' \left( \textbf{y} - \tilde{\textbf{M}} \odot \textbf{p} \right)}
\end{aligned}
\end{equation}
where $\odot$ represents the Hadamard product (term-by-term multiplication).

\paragraph{Hessian of log-likelihood: \\}
\begin{equation}
\begin{aligned}
	& \frac{\partial^2 LL}{\partial \beta_l \partial \beta_{l'}} && = - \sum_{n,j} \tilde{M}_{nj} p_{nj} X_{njl} X_{njl'} + \sum_n M_n A_{nl} A_{nl'} \\
	\text{where } \hspace*{5pt} & A_{nl} && =\sum_j p_{nj} X_{njl} \\
 \implies	& \textbf{H}(\beta) && = \boxed{\textbf{X}' \left[ \textbf{X} \odot \left(\textbf{h} \cdot \mathbbm{1}_L \right) \right]  + \textbf{A}' \left[ \textbf{A}  \odot \left(\textbf{M} \cdot \mathbbm{1}_L \right) \right] }  \\ % TO DO
  \text{where} \hspace*{5pt} & \textbf{h} && = - \tilde{\textbf{M}} \odot \textbf{p}
\end{aligned}
\end{equation}


\subsection{Estimation by Maximum Likelihood}
Computational trick:
\begin{equation}
\begin{aligned}
	& \log(p_{nj}) && = V_{nj} - \log\left( \sum_k \exp(V_{nk}) \right) \\
	&  && = V_{nj} - \bar{V}_n - \log\left( \sum_k \exp(V_{nk} - \bar{V}_n) \right) \\
\text{where } \hspace*{5pt} & \bar{V}_n && = \underset{j}{\max} \hspace*{5pt} V_{nj}
\end{aligned}
\end{equation}
This trick avoids overflow issues that arise when computing the exponential of large values.

\end{document}

